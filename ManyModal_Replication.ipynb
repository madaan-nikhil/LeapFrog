{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ManyModal_Replication",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Drive Misc"
      ],
      "metadata": {
        "id": "0dqygAO8IVOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKk_BejH9lPJ"
      },
      "outputs": [],
      "source": [
        "!sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!sudo apt-get update -qq 2>&1 > /dev/null\n",
        "!sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "!google-drive-ocamlfuse\n",
        "\n",
        "!sudo apt-get install -qq w3m # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "path = \"/content/drive/MyDrive/18-786-Intro-DL/IDL-Project/WebQA_data_first_release/WebQA_train_val.json\"\n",
        "q = json.load(open(path,'r'))\n"
      ],
      "metadata": {
        "id": "0UBFD6bX98Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EY3IpmQg6IfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and imports"
      ],
      "metadata": {
        "id": "O81cldlVIZTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import clip\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "%pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "\n",
        "class QADataset(Dataset):\n",
        "  def __init__(self,  data):\n",
        "    self.data = data\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "    self.instances = list(self.data.keys())\n",
        "    self.Qs = [self.tokenizer(self.data[instance]['Q'], \n",
        "                               padding='max_length', max_length = 512, truncation=True,\n",
        "                                return_tensors=\"pt\") for instance in self.instances]\n",
        "    # print(self.Qs[6])\n",
        "    self.lens = [(Q['input_ids'] == 0).nonzero(as_tuple=False)[0][1].numpy() for Q in self.Qs]\n",
        "    self.modals = [0 if len(self.data[instance]['img_posFacts']) else 1 for instance in self.instances]\n",
        "\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.instances)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.Qs[idx], self.lens[idx], self.modals[idx]\n",
        "    \n",
        "# path = \"/content/drive/MyDrive/18-786-Intro-DL/IDL-Project/WebQA_data_first_release/WebQA_train_val.json\"\n",
        "# dataset = QADataset(path)\n",
        "\n"
      ],
      "metadata": {
        "id": "vBfwF1kX-CJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader Generation"
      ],
      "metadata": {
        "id": "4u9kHvNiIciH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "train_path = \"/content/drive/MyDrive/18-786-Intro-DL/IDL-Project/WebQA_data_first_release/WebQA_train_val.json\"\n",
        "q = json.load(open(train_path,'r'))\n",
        "# test_path = \"/content/drive/MyDrive/18-786-Intro-DL/IDL-Project/WebQA_data_first_release/WebQA_test.json\"\n",
        "# qq =  json.load(open(test_path,'r'))\n",
        "\n",
        "val_pts = {k:v for k,v in q.items() if v['split'] == 'val'}\n",
        "train_pts = {k:v for k,v in q.items() if v['split'] == 'train'}\n",
        "\n",
        "train_loader = DataLoader(QADataset(train_pts),batch_size=128,shuffle=True)\n",
        "val_loader = DataLoader(QADataset(val_pts),batch_size=128,shuffle=False)\n",
        "# test_loader = DataLoader(QADataset(qq),batch_size=128,shuffle=False)"
      ],
      "metadata": {
        "id": "wO-3MTG1-EZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache() # Use this often\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "xjTQ0xpn-Flo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "\n",
        "This model was used to initially try out manymodal as described [in this paper](https://arxiv.org/abs/2001.08034)"
      ],
      "metadata": {
        "id": "Z0BHA9brIh5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import BertModel, BertConfig\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "configuration = BertConfig('bert-base-cased')\n",
        "bert = BertModel.from_pretrained('bert-base-cased').cuda().eval()\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "        \n",
        "        # print(configuration.hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.reducer = nn.Linear(768,256)\n",
        "        self.lstm = nn.LSTM(input_size=256, hidden_size=512,\\\n",
        "                            num_layers=2, bidirectional=True, dropout=.2)\n",
        "        # self.relu = nn.ReLU()\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(512*2,1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=.2),\n",
        "            nn.Linear(1024,512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=.2),\n",
        "            nn.Linear(512,2)\n",
        "        )\n",
        "\n",
        "    def forward(self, seq_output, lens):\n",
        "        batch_size = seq_output[0].shape[0]\n",
        "        # print(batch_size)\n",
        "        # with torch.no_grad():\n",
        "        #    = self.\n",
        "        # print(seq_output[0].shape)\n",
        "        dropout_output = self.dropout(self.reducer(seq_output[0]))\n",
        "        packed_input = pack_padded_sequence(dropout_output, lengths=lens, batch_first=True, enforce_sorted=False)\n",
        "        out1, (out2, out3) = self.lstm(packed_input)\n",
        "        out, lengths  = pad_packed_sequence(out1,batch_first=True)\n",
        "        # print(out.shape, lens,lengths,out[:,-1,:].view(batch_size,-1).shape)\n",
        "        final_layer = self.linear(out[:,-1,:].view(batch_size,-1))\n",
        "\n",
        "        return final_layer\n",
        "\n",
        "model = BertClassifier().cuda()\n",
        "try:\n",
        "  import torchsummaryX\n",
        "except:\n",
        "  !pip install torchsummaryX\n",
        "  import torchsummaryX\n",
        "from torchsummaryX import summary\n",
        "\n",
        "x = next(iter(train_loader))\n",
        "with torch.no_grad():\n",
        "  seq_output =  bert(input_ids= x[0]['input_ids'].squeeze().cuda(), attention_mask=x[0]['attention_mask'].cuda(),return_dict=False)\n",
        "summary(model, seq_output, x[1])"
      ],
      "metadata": {
        "id": "RXKi_Ir2-HaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer and Losses"
      ],
      "metadata": {
        "id": "BWTkY-MpI2nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=1e-3, weight_decay=4e-6)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "_2uKcpcN-JD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Loops"
      ],
      "metadata": {
        "id": "lYf-XJ1JI6vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def validate():\n",
        "  model.eval()\n",
        "  batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "  num_correct = 0\n",
        "  total_loss = 0.\n",
        "  for i , (q, lens, target) in enumerate(val_loader):\n",
        "    # print(q['input_ids'].shape)\n",
        "    input_id = q['input_ids'].squeeze().cuda()\n",
        "    mask = q['attention_mask'].cuda()\n",
        "    with torch.no_grad():\n",
        "      seq_output =  bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "    output = model(seq_output,lens)\n",
        "    loss = criterion(output, target.cuda())\n",
        "   \n",
        "    num_correct += int((torch.argmax(output, axis=1) == target.cuda()).sum())\n",
        "\n",
        "    batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * 128)))\n",
        "    batch_bar.update()\n",
        "  val_loss = total_loss / len(val_loader)   \n",
        "  batch_bar.close()\n",
        "  print(\"\\n\")\n",
        "  print(\"Validation: {:.04f}%\".format(100 * num_correct / (len(val_loader)*128)))\n",
        "  return num_correct/(len(val_loader)*128) *100, val_loss\n",
        "\n",
        "for epoch in range(50):\n",
        "  batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "  num_correct = 0\n",
        "  total_loss = 0\n",
        "  model.train()\n",
        "  for i , (q, lens, target) in enumerate(train_loader):\n",
        "    # print(q['input_ids'].shape)\n",
        "    input_id = q['input_ids'].squeeze().cuda()\n",
        "    mask = q['attention_mask'].cuda()\n",
        "    with torch.no_grad():\n",
        "      seq_output =  bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "    output = model(seq_output,lens)\n",
        "    loss = criterion(output, target.cuda())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    num_correct += int((torch.argmax(output, axis=1) == target.cuda()).sum())\n",
        "    total_loss += float(loss)\n",
        "    batch_bar.set_postfix(\n",
        "              acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * 128)),\n",
        "              loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "              num_correct=num_correct,\n",
        "              lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "    batch_bar.update() \n",
        "\n",
        "  batch_bar.close()\n",
        "  acc = 100 * num_correct / (len(train_loader) * 128)\n",
        "  tr_loss = float(total_loss / len(train_loader))\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        50,\n",
        "        acc,\n",
        "        tr_loss))\n",
        "  val_acc,val_loss = validate()\n",
        "  print(f\"valid Acc: {val_acc}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Qarhn8t-KXB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}