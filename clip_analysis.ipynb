{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f168272d",
   "metadata": {},
   "source": [
    "# Source retrieval with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c5b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "print(clip.available_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9283ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.to(device).eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length   = model.context_length\n",
    "vocab_size       = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\",   context_length)\n",
    "print(\"Vocab size:\",       vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff248d",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c43410",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/WebQA_train_val.json') as f:\n",
    "    json_dict = json.load(f)\n",
    "\n",
    "json_dict['d5bbc6d80dba11ecb1e81171463288e9'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756406ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/val_subset_1644.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43ff04",
   "metadata": {},
   "source": [
    "## Compute similarities\n",
    "### Text and Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text_cos_sim = []\n",
    "neg_text_cos_sim = []\n",
    "\n",
    "total_qa = 1000\n",
    "batch_bar = tqdm(total=total_qa, dynamic_ncols=True, desc='Text-Q Similarity') \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (key, qa) in enumerate(data.items()):\n",
    "        if i == total_qa:\n",
    "            break\n",
    "        # Encode question\n",
    "        question          = qa['Q']\n",
    "        question_tokens   = clip.tokenize([question])\n",
    "        question_features = model.encode_text(question_tokens).float()\n",
    "        question_features /= question_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Encode positive images\n",
    "        if len(qa['TxtPos']) > 0:\n",
    "            pos_text_tokens   = clip.tokenize( [txt[:60] for txt in qa['TxtPos']] )\n",
    "            pos_text_features = model.encode_text(pos_text_tokens)\n",
    "            pos_text_features /= pos_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # Compute similarity\n",
    "            pos_text_similarity = torch.matmul(question_features, pos_text_features.T).squeeze(0)\n",
    "            pos_text_cos_sim.extend(pos_text_similarity.tolist())\n",
    "\n",
    "        # Encode distractor images\n",
    "        if len(qa['TxtNeg']) > 0:\n",
    "            neg_text_tokens   = clip.tokenize( [txt[:60] for txt in qa['TxtNeg']] )\n",
    "            neg_text_features = model.encode_text(neg_text_tokens)\n",
    "            neg_text_features /= neg_text_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Compute similarity\n",
    "            neg_text_similarity = torch.matmul(question_features, neg_text_features.T).squeeze(0)\n",
    "            neg_text_cos_sim.extend(neg_text_similarity.tolist())\n",
    "\n",
    "        batch_bar.set_postfix(Avg_pos=\"{:.3f}\".format(np.mean(pos_text_cos_sim)),\n",
    "                              Avg_neg=\"{:.3f}\".format(np.mean(neg_text_cos_sim)))\n",
    "        batch_bar.update()\n",
    "\n",
    "batch_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af12678",
   "metadata": {},
   "source": [
    "### Captions and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_caption_cos_sim = []\n",
    "neg_caption_cos_sim = []\n",
    "\n",
    "total_qa = 1000\n",
    "batch_bar = tqdm(total=total_qa, dynamic_ncols=True, desc='Caption-Q Similarity') \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (key, qa) in enumerate(json_dict.items()):\n",
    "        if i == total_qa:\n",
    "            break\n",
    "        # Encode question\n",
    "        question          = qa['Q']\n",
    "        question_tokens   = clip.tokenize([question])\n",
    "        question_features = model.encode_text(question_tokens).float()\n",
    "        question_features /= question_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Encode positive images\n",
    "        if len(qa['img_posFacts']) > 0:\n",
    "            pos_caption_tokens   = clip.tokenize( [txt['caption'][:60] for txt in qa['img_posFacts']] )\n",
    "            pos_caption_features = model.encode_text(pos_caption_tokens)\n",
    "            pos_caption_features /= pos_caption_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # Compute similarity\n",
    "            pos_caption_similarity = torch.matmul(question_features, pos_caption_features.T).squeeze(0)\n",
    "            pos_caption_cos_sim.extend(pos_caption_similarity.tolist())\n",
    "\n",
    "        # Encode distractor images\n",
    "        if len(qa['img_negFacts']) > 0:\n",
    "            neg_caption_tokens   = clip.tokenize( [txt['caption'][:60] for txt in qa['img_negFacts']] )\n",
    "            neg_caption_features = model.encode_text(neg_caption_tokens)\n",
    "            neg_caption_features /= neg_caption_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Compute similarity\n",
    "            neg_caption_similarity = torch.matmul(question_features, neg_caption_features.T).squeeze(0)\n",
    "            neg_caption_cos_sim.extend(neg_caption_similarity.tolist())\n",
    "\n",
    "        batch_bar.set_postfix(Avg_pos=\"{:.3f}\".format(np.mean(pos_caption_cos_sim)),\n",
    "                              Avg_neg=\"{:.3f}\".format(np.mean(neg_caption_cos_sim)))\n",
    "        batch_bar.update()\n",
    "\n",
    "batch_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f6f20",
   "metadata": {},
   "source": [
    "### Title and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5648c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_title_cos_sim = []\n",
    "neg_title_cos_sim = []\n",
    "\n",
    "total_qa = 1000\n",
    "batch_bar = tqdm(total=total_qa, dynamic_ncols=True, desc='Title-Q Similarity') \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (key, qa) in enumerate(json_dict.items()):\n",
    "        if i == total_qa:\n",
    "            break\n",
    "        # Encode question\n",
    "        question          = qa['Q']\n",
    "        question_tokens   = clip.tokenize([question])\n",
    "        question_features = model.encode_text(question_tokens).float()\n",
    "        question_features /= question_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Encode positive images\n",
    "        if len(qa['img_posFacts']) > 0:\n",
    "            pos_title_tokens   = clip.tokenize( [txt['title'][:60] for txt in qa['img_posFacts']] )\n",
    "            pos_title_features = model.encode_text(pos_title_tokens)\n",
    "            pos_title_features /= pos_title_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # Compute similarity\n",
    "            pos_title_similarity = torch.matmul(question_features, pos_title_features.T).squeeze(0)\n",
    "            pos_title_cos_sim.extend(pos_title_similarity.tolist())\n",
    "\n",
    "        # Encode distractor images\n",
    "        if len(qa['img_negFacts']) > 0:\n",
    "            neg_title_tokens   = clip.tokenize( [txt['title'][:60] for txt in qa['img_negFacts']] )\n",
    "            neg_title_features = model.encode_text(neg_caption_tokens)\n",
    "            neg_title_features /= neg_title_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Compute similarity\n",
    "            neg_title_similarity = torch.matmul(question_features, neg_title_features.T).squeeze(0)\n",
    "            neg_title_cos_sim.extend(neg_title_similarity.tolist())\n",
    "\n",
    "        batch_bar.set_postfix(Avg_pos=\"{:.3f}\".format(np.mean(pos_title_cos_sim)),\n",
    "                              Avg_neg=\"{:.3f}\".format(np.mean(neg_title_cos_sim)))\n",
    "        batch_bar.update()\n",
    "\n",
    "batch_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2841d",
   "metadata": {},
   "source": [
    "### Images and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572484ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_image_cos_sim = []\n",
    "neg_image_cos_sim = []\n",
    "\n",
    "total_qa = 1000\n",
    "batch_bar = tqdm(total=total_qa, dynamic_ncols=True, desc='Image-Q Similarity') \n",
    "\n",
    "img_shape = (224,224,3)\n",
    "with torch.no_grad():\n",
    "    for i, (key, qa) in enumerate(data.items()):\n",
    "        if i == total_qa:\n",
    "            break\n",
    "        # Encode question\n",
    "        question          = qa['Q']\n",
    "        question_tokens   = clip.tokenize([question])\n",
    "        question_features = model.encode_text(question_tokens).float()\n",
    "        question_features /= question_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Encode positive images\n",
    "        if len(qa['ImgPos']) > 0:\n",
    "            # Get indices where shape is correct\n",
    "            idx = np.where([np.shape(img)==img_shape for img in qa['ImgPos']])[0]\n",
    "            if len(idx) > 0:\n",
    "                pos_image_input    = torch.tensor(np.stack([ qa['ImgPos'][i] for i in idx])).permute([0,3,1,2])\n",
    "                pos_image_features = model.encode_image(pos_image_input).float()\n",
    "                pos_image_features /= pos_image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                # Compute similarity\n",
    "                pos_image_similarity = torch.matmul(question_features, pos_image_features.T).squeeze(0)\n",
    "                pos_image_cos_sim.extend(pos_image_similarity.tolist())\n",
    "\n",
    "        # Encode distractor images\n",
    "        if len(qa['ImgNeg']) > 0:\n",
    "            # Get indices where shape is correct\n",
    "            idx = np.where([np.shape(img)==img_shape for img in qa['ImgNeg']])[0]\n",
    "            if len(idx) > 0:\n",
    "                neg_image_input    = torch.tensor(np.stack([ qa['ImgNeg'][i] for i in idx])).permute([0,3,1,2])\n",
    "                neg_image_features = model.encode_image(neg_image_input).float()\n",
    "                neg_image_features /= neg_image_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "                # Compute similarity\n",
    "                neg_image_similarity = torch.matmul(question_features, neg_image_features.T).squeeze(0)\n",
    "                neg_image_cos_sim.extend(neg_image_similarity.tolist())\n",
    "\n",
    "        batch_bar.set_postfix(Avg_pos=\"{:.3f}\".format(np.mean(pos_image_cos_sim)), Avg_neg=\"{:.3f}\".format(np.mean(neg_image_cos_sim)))\n",
    "        batch_bar.update()\n",
    "\n",
    "batch_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0cbec",
   "metadata": {},
   "source": [
    "## Similarity histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68351c0b",
   "metadata": {},
   "source": [
    "### Images and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(pos_image_cos_sim, bins=30, color=[4/255.0,   101/255.0, 130/255.0], alpha=0.6, ax=ax, kde=True, stat='density')\n",
    "sns.histplot(neg_image_cos_sim, bins=30, color=[243.0/255, 145.0/255, 137/255.0], alpha=0.6, ax=ax, kde=True, stat='density')\n",
    "ax.set_xlabel(\"Cosine similarity with the question\")\n",
    "plt.setp(ax.patches, linewidth=0.2);\n",
    "ax.legend([\"Positive images\", \"Negative images\"])\n",
    "plt.savefig('image_question_cos_distance_subset.pdf', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d369a",
   "metadata": {},
   "source": [
    "### Texts and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65762a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(pos_text_cos_sim, bins=30, color=[4/255.0,   101/255.0, 130/255.0], alpha=0.6, ax=ax, kde=True, stat='density')\n",
    "sns.histplot(neg_text_cos_sim, bins=40, color=[243.0/255, 145.0/255, 137/255.0], alpha=0.6, ax=ax, kde=True, stat='density')\n",
    "ax.set_xlabel(\"Cosine similarity with the question\")\n",
    "plt.setp(ax.patches, linewidth=0.2);\n",
    "ax.legend([\"Positive texts\", \"Negative texts\"])\n",
    "plt.savefig('text_question_cos_distance_subset.pdf', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2671527",
   "metadata": {},
   "source": [
    "### Captions and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(pos_caption_cos_sim, bins=24, color=[4/255.0,   101/255.0, 130/255.0], alpha=0.6, ax=ax, kde=True, stat='density')\n",
    "sns.histplot(neg_caption_cos_sim, bins=32, color=[243.0/255, 145.0/255, 137/255.0], alpha=0.6, ax=ax, kde=True, stat='density')\n",
    "ax.set_xlabel(\"Cosine similarity with the question\")\n",
    "plt.setp(ax.patches, linewidth=0.2);\n",
    "ax.legend([\"Positive image captions\", \"Negative image captions\"])\n",
    "plt.savefig('caption_question_cos_distance_subset.pdf', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48360c1d",
   "metadata": {},
   "source": [
    "### Titles and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(pos_title_cos_sim, bins=38, color=[4/255.0,   101/255.0, 130/255.0], alpha=0.6, ax=ax, kde=True, stat='density')\n",
    "sns.histplot(neg_title_cos_sim, bins=55, color=[243.0/255, 145.0/255, 137/255.0], alpha=0.6, ax=ax, kde=True, stat='density')\n",
    "ax.set_xlabel(\"Cosine similarity with the question\")\n",
    "plt.setp(ax.patches, linewidth=0.2);\n",
    "ax.legend([\"Positive image titles\", \"Negative image titles\"])\n",
    "plt.savefig('title_question_cos_distance_subset.pdf', transparent=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b4573f74ddebefe888557a45480cb32406e795127ef77fd25190bb90cfd997a"
  },
  "kernelspec": {
   "display_name": "alfred_kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
